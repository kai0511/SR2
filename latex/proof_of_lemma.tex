\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathptmx}
% space margins
\usepackage[margin=1in]{geometry}
% set font style
\usepackage[mono=false]{libertine}
% reference style
\usepackage[style=numeric]{biblatex}
\addbibresource{reference.bib}

\DeclareMathOperator{\sign}{sign}
\newtheorem{lemma}{LEMMA}
\renewcommand\thelemma{\unskip}
\renewcommand\thelemma{\unskip}

\begin{document}
\title{Proof of the Lemma}
%% \author{Zhao, Kai}
%% \date{\today}
\maketitle

\section*{}
\begin{lemma} 
    Let $\{X_i\}_{i=1,\cdots,n}$ be independent and identically distributed with weighted mean $\mu_w$ and variance $\sigma_w^2$ by known sample weights $\{ w_i \}_{i=1,\cdots,n}$, with $\mathrm{E}(wX^4) < \infty$ and $g(x)$ a continuous function for $x > 0$, then 
    \begin{equation}
        \sqrt{n}(\log s_n^2 - \log \sigma_w^2) \xrightarrow{\text{d}} N(0, \frac{\mu_4 - \sigma_w^4}{\sigma_w^4}),
    \end{equation}
    where $s_w^2=(w_0 - 1)^{-1} \sum_{i=1}^n w_i (X_i - \bar{X}_w)^2$ and $\mu_4 = \mathrm{E} [ w(X - \mu_w)^4 ]$. Here $w_0 = \sum_i w_i$ and $\bar{X}_w = \sum_i w_i X_i / w_0$.
\end{lemma}

\begin{proof} 
Consider the weighted sample variance
\begin{equation*}
    \begin{split}
        (w_0 - 1)s^2 & = \sum_{i=1}^n w_i \Big((X_i-\mu_w) - (\bar X-\mu_w)\Big)^2 \\
        & =\sum_{i=1}^n w_i \Big(X_i -\mu_w \Big)^2 - 2\sum_{i=1}^n w_i \Big((X_i-\mu_w)(\bar X -\mu_w)\Big)+\sum_{i=1}^n w_i \Big(\bar X_w -\mu_w \Big)^2 \\
        & =\sum_{i=1}^n w_i \Big(X_i -\mu_w \Big)^2 - 2 w_0 \Big(\sum_{i=1}^n \frac{w_i}{w_0}(X_i-\mu_w)(\bar X -\mu_w)\Big)+\sum_{i=1}^n w_i \Big(\bar X_w -\mu_w \Big)^2 \\
        & =\sum_{i=1}^n w_i \Big(X_i-\mu_w \Big)^2 - 2 w_0\Big(\bar X_w - \mu_w \Big)^2 + w_0\Big(\bar X_w - \mu_w \Big)^2 \\
        & = \sum_{i=1}^n w_i \Big(X_i-\mu_w \Big)^2 - w_0\Big(\bar X_w - \mu_w \Big)^2
    \end{split}
\end{equation*}
Therefore, we have 
\begin{equation*}
    \sqrt w_0 (s_w^2 - \sigma_w^2) = \frac {\sqrt w_0}{w_0 - 1}\sum_{i=1}^n w_i \Big(X_i-\mu_w\Big)^2 -\sqrt w_0 \sigma_w^2-  \frac {\sqrt w_0}{w_0-1} w_0 \Big(\bar X_w -\mu_w\Big)^2,
\end{equation*}
and the above equation can be manipulated into
\begin{equation}
    \label{eqn:manipulated_eqn}
    \begin{split}
        \sqrt w_0 (s_w^2 - \sigma_w^2) & = \frac{w_0 \sqrt w_0}{w_0 -1}\frac 1 w_0\sum_{i=1}^n w_i \Big(X_i - \mu_w \Big)^2 -\sqrt w_0 \frac {w_0 - 1}{w_0 - 1}\sigma_w^2-  \frac {w_0}{w_0-1}\sqrt w_0 \Big(\bar X_w -\mu_w\Big)^2 \\
        & =\frac {\sqrt w_0}{w_0 - 1}\left[\sum_{i=1}^n w_i \left((X_i-\mu_w)^2 - \sigma_w^2\right)\right] + \frac {\sqrt w_0}{w_0 - 1}\Big(\sigma_w^2 -  w_0 (\bar X_w -\mu_w)^2 \Big).
    \end{split}
\end{equation}
On the one hand, by Chebyshev's Inequality, one can derive that
\begin{equation*}
    \begin{split}
        & \Pr\Bigg(\Bigg|\frac {\sqrt w_0}{w_0-1}\Big(\sigma_w^2 -  w_0 (\bar X_w -\mu_w)^2 \Big)\Bigg| > \epsilon \Bigg)  \\
        & = \Pr\Bigg(\Bigg|\frac {1}{\sqrt w_0(w_0-1)}\Big(w_0 \sigma_w^2 -  w_0^2 (\bar X_w -\mu_w)^2 \Big)\Bigg| > \epsilon \Bigg) \\
        & = \Pr\Bigg(\Bigg|\frac {1}{\sqrt w_0(w_0-1)}\Big(w_0 \sigma_w^2 -  ( \sum_i w_i X_i - w_0 \mu_w)^2 \Big)\Bigg| > \epsilon \Bigg) \\
        & \leq \frac{\mathrm{E}\bigg[ \bigg( \sum_i w_i (X_i - \mu_w)^2 - ( \sum_i w_i X_i - w_0 \mu_w)^2 \bigg)^2 \bigg]}{\epsilon^2 w_0 (w_0 - 1)^2} \\
        & \leq \frac{\mathrm{E}\bigg[ \sum_i w_i (X_i - \mu_w)^2 \bigg]^2 }{\epsilon^2 w_0 (w_0-1)^2} \\
        & = \frac{w_0 \mu_4^w + \frac{n(n-1)}{2} \sigma_w^4}{{\epsilon^2 w_0 (w_0-1)^2}} \rightarrow 0,
    \end{split}
\end{equation*}
hence 
\begin{equation*}
    \frac {\sqrt n}{n-1}\Big(\sigma^2 -  n (\bar X -\mu)^2 \Big) \xrightarrow{P} 0.
\end{equation*}

\noindent On the other hand, since we have
\begin{equation*}
    \begin{split}
        \mathrm{E}_w \Big[w_i(X_i-\mu_w)^2 \Big] & = w_i \sigma_w^2,\\
        \operatorname {Var}\left[ w_i \Big(X_i-\mu_w \Big)^2\right] & = \mathrm{E} \Big[ w_i^2 (X_i-\mu_w)^4 \Big] -  \mathrm{E} ^2 \Big[ w_i (X_i-\mu_w) \Big]^2 \\ 
        & = w_i \mu_4 - \sigma^4,
    \end{split}
\end{equation*}
one can conclude the following equation
\begin{equation*}
    \left[\sqrt w_0 \left(\frac 1 w_0 \sum_{i=1}^n w_i \Big(X_i-\mu\Big)^2 -\sigma_w^2\right)\right] \sim  N\left(0,\mu_4 - \sigma^4\right).
\end{equation*}
Thus, under Central Limit Theorem, we have
\begin{equation}
    \sqrt n(s^2 - \sigma^2) \xrightarrow{d} N\left(0,\mu_4 - \sigma^4\right), 
\end{equation}
and then Delta-Method \cite{Larry2020all} would lead us to 
\begin{equation*}
    \begin{split}
        \sqrt{n}(\log s_n^2 - \log \sigma^2) & \xrightarrow{\text{d}} g \prime (\sigma^2) \cdot N\left(0,\mu_4 - \sigma^4\right) \\
        & \stackrel{\text{d}}{=} \frac{1}{\sigma^2} N(0,\mu_4 - \sigma^4) \\
        & \stackrel{\text{d}}{=} N(0, \frac{\mu_4 - \sigma^4}{\sigma^4}).
    \end{split}
\end{equation*}
\end{proof}

\printbibliography
\end{document}