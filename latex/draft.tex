\documentclass[12pt]{article}

% set font style
\usepackage{times}

% \usepackage{mathptmx}
\usepackage{amssymb}
% \usepackage{mathalfa}
\usepackage{amsmath}
\usepackage{mathtools}

\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\tr}{tr}
\usepackage{bbold}
% line spacing
% \usepackage{setspace}
% \setstretch{1.25}
\renewcommand{\baselinestretch}{1.25}
\setlength{\parskip}{0.5em}
% appendix
\usepackage[titletoc]{appendix}
% space margins
\usepackage[margin=1in]{geometry}

% \usepackage{algorithm}       % http://ctan.org/pkg/algorithm
% \usepackage{algpseudocode}   % http://ctan.org/pkg/algorithmicx
\usepackage[ruled,linesnumbered]{algorithm2e}

% reference style
\usepackage[style=ieee]{biblatex}
\addbibresource{reference.bib}

\DeclareMathOperator{\sign}{sign}

\begin{document}
\title{SC2: Sparse Coding for Large Scale Single-cell RNA Sequencing Data Analysis}
% \author{Zhao, Kai}
\date{}
\maketitle

\section{Motivations}
  Single-cell RNA-sequencing (scRNA-seq) technologies enable us measure transcriptomic level of individual cells. In scRNA-seq experiments, scRNA expression data usually comes from different individuals with different phenotypes across different conditions and even species using different technologies \cite{butler2018integrating}. This may lead to severe batch effects and introduction of heterogeneous biological variation in scRNA-seq data, making downstream analysis challenging. Most existing approach are proposed to address technical variations caused by different technologies. For example, statistical approaches, such as Seurat \cite{butler2018integrating} and LIGER \cite{liu2020jointly}, were proposed to integrate multiple single cell datasets from different protocols and data modalities. However, few statistical approaches focus on variations from heterogeneous biological variations. scINSIGHT \cite{qian2022scinsight} is probably the first one on this issue. Based on nonnegative matrix factorization (NMF), scINSIGHT was proposed to model the variations from heterogeneous biological conditions \cite{qian2022scinsight}. However, previous study \cite{stein2018enter} pointed out that NMF cannot directly infer down-regulations of biological entities in biological data.

  We propose a novel statistical approach, SC2, based on additive matrix factorization to model heterogeneous biological variations introduced by different biological variables (e.g., donor, tissue, and disease status) and to decompose cellular variations across single-cell samples into a low-rank latent space simultaneously. SC2 restricts the heterogeneous biological variations in a shared latent space, thus enjoying a better interpretability. This strategy is consistent with our understanding. Specifically, different donors demonstrate heterogeneity in gene expression via some pathways, and different phenotypes of these donors can affect the expression of genes via the same pathways. Meanwhile, we can easily observe that different donors can also be affected differently by the same phenotypes. Thus, to address the concern, SC2 incorporates interactions between different biologically variables.

\section{Methods}
  Here we propose a novel statistical approach, SC2, to decompose cellular variations of scRNA-Seq samples across multiple biological variables (e.g., donor, tissue, and other biological conditions) into low-rank latent spaces to facilitate downstream analysis. In SC2, we integrate matrix factorization, which is utilized to capture variation from the biological variables, and dictionary learning to learn low-rank representations for each cell. In the employment of dictionary learning, we introduce the sparse penalty on latent representations for cells to facilitate cell population discovery and norm constraints on the corresponding gene representations to ensure an equal scaling for each latent components.

  \subsection{Model specifications}
    For illustration, let $Z^{N \times M}$ denote the matrix of log-normalized scRNA-Seq expression levels of $N$ samples of $M$ genes. The $N$ samples originate from several biological conditions (e.g., individuals, phenotypes, tissues or disease phases). Here we use donor and phenotype as examples for demonstration. These samples come from $N_1$ donors with $N_2$ phenotypes. Thus, the expression level of gene $m$ from sample $i$, which is obtained from donor $j$ with phenotype $t$, $z_{im}$, can be modelled as
    \begin{equation}
      \label{eqn::sc_rna_eqn}
        \hat{z}_{im} \approx d_{j}^T v_m + p_{t}^T v_m + s_i^T g_m
    \end{equation}
    where $d_j, p_t, v_m$ are vectors of length $K_1$, $s_i, g_m$ are vectors of length $K_2$. The donor and phenotype information of sample $i$ is known. In the above equation, the first two terms in the right capture variation from donor and phenotype, which is restricted in a shared low-rank latent space. The last term in the right seeks to decompose scRNA-Seq samples into a different low-rank latent space after controlling  variation from donor and phenotype. In SC2, the biological variables depend on needs in applications. 

    The objective function for Equation \ref{eqn::sc_rna_eqn} is formulated as
    \begin{equation}
      \label{eqn::sc_rna_obj}
      \begin{split}
        \mathcal{L}(d, p, v, s, g) = & \frac{1}{2} \sum_{i,m}  \big( z_{im} - d_j v_m^T - p_t v_m^T - s_i^T g_m \big)^2 + \\ 
        & \frac{1}{2} \lambda_1 (\sum_j ||d_j||_2^2 + \sum_t ||p_t||_2^2 + \sum_m ||v_m||_2^2 ) + \\ 
        & \lambda_2 \big( \frac{1}{2} (1 - \alpha) \sum_i ||s_i||_2^2 + \alpha \sum_i ||s_i||_1 \big) \\
        \mathrm{subject\ to} \qquad & \sum_m g_{mk}^2 \leq c, \forall k = 1, \dots, K_2
      \end{split}
    \end{equation}
    In the above equation, $g_{mk}$ is the $k$-th element of $g_m$, and $c$ is a constant (usually 1). We introduce the elastic net penalty on the cell representation $s_i$ to encourage sparsity to facilitate cell clustering. Moreover, the norm constrain is introduced to ensure the same scale for each component in decomposing cellular variation.
    
    The Equation \ref{eqn::sc_rna_obj} can be represented with matrix operation. Let $D^{N_1 \times K_1}, P^{N_2 \times K_1}, V^{K_1 \times M}$ are matrices of latent representations of $N_1$ donors, $N_2$ phenotypes, and $M$ genes, respectively, and denote $S^{N \times K_2}, G^{K_2 \times M}$ latent representations for $N$ cells and $M$ genes after controlling  variation from donor and phenotype. The matrix representation of Equation \ref{eqn::sc_rna_obj} can be written as follows:
    \begin{equation}
      \label{eqn::sc_rna_obj_mat}
      \begin{split}
        \mathcal{L} (D, P, V, S, G) = & \frac{1}{2} \big \| Z -  (X_D D + X_P P) V -  S G \big \|_F^2 + \\
        & \frac{1}{2} \lambda_1 (\|D\|_F^2 + \|P\|_F^2 + \|V\|_F^2) + \\
        & \lambda_2 \big[ \frac{1}{2} (1 - \alpha) \|S\|_F^2 + \alpha \|S\|_1 \big], \\
        \mathrm{subject\ to} \qquad & \|G_k\|_2^2 \leq c, \forall k = 1, \dots, K_2,
      \end{split}
    \end{equation}
    where $X_D^{N \times N_1}, X_P^{N \times N_2}$ are indicator matrices, which represent the dummy variables for the samples, $G_k$ is the $k$-th row of matrix $G$, and $c$ is a constant, which restricts the scaling of each component of $G$.
  
\subsection{Model fitting}
  Alternating block coordinate descent (BCD) was employed to optimize Equation \ref{eqn::sc_rna_obj}. Practically, each time we update a set of independent parameter with all other parameters fixed. For example, when all parameters except $v_m$ are fixed, then our problem becomes a number of linear regression problems with ridge regularization and all $v_m$ can be updated in parallel. In each iteration of BCD, we update each set of parameters of our model sequentially and repeat the process until the stopping criteria meets.

  \subsubsection*{Optimize with the whole data}
    With the objective function defined by Equation \ref{eqn::sc_rna_obj} and notations defined in Equation \ref{eqn::sc_rna_obj_mat}, we can easily derive a closed form for updating $d_j, p_t, v_m$ in optimization. First, we have the following update for $v_m$
    \begin{equation}
      \label{eqn::updating_vm}
      v_m = \big[ W^T W + \lambda_1 \mathbb{I}_{K_1} \big]^{-1} W^T \tilde{Z}_m,
    \end{equation}
    where $W = X_D D + X_P P$, $\tilde{Z} = Z - S G$, and $\tilde{Z}_m$ is the $m$-th column of $\tilde{Z}$. Similarly, the update for $d_j$ is 
    \begin{equation}
      \label{eqn::updating_dj}
      d_j = \big[ N_j \sum_m v_m v_m^T + \lambda \mathbb{I}_{K_1} \big]^{-1} \sum_{i \in B_j} \sum_m \hat{z}_{im} v_m,
    \end{equation}
    where $\tilde{Z} = Z - S G - X_P P V$, $B_j$ is the set of indices of samples from donor $j$, and $N_j$ is the number of elements in $B_j$ Likewise, the update for $p_t$ is 
    \begin{equation}
      \label{eqn::updating_pt}
      p_t = \big[ N_t \sum_m v_m v_m^T + \lambda \mathbb{I}_{K_1} \big]^{-1} \sum_{i \in B_t} \sum_m \hat{z}_{im} v_m,
    \end{equation}
    where $\tilde{Z} = Z - S G - X_D D V$, $B_t$ is the set of indices of samples from donors with phenotype $t$, and $N_t$ is the number of elements in $B_t$.
    
    When optimizing the object function defined by Equation \ref{eqn::sc_rna_obj_mat} with respect to $G$, the Lagrange dual proposed in the study \cite{lee2006efficient} is employed. The Lagrange dual for our problem is of the following form
    \begin{equation*}
      \label{eqn::Lagrangian}
      \begin{split}
        \mathcal{L} (G, \vec{\psi}) & = \frac{1}{2} \tr \big( G^\intercal Q G \big) - \tr \big( W G \big) + \frac{1}{2} \tr(\Psi G G^\intercal - c \Psi) + const.
      \end{split}
    \end{equation*}
    Here $\tilde{Z} = Z - X_D D V - X_P P V $, $Q=S^\intercal S$, $W=\tilde{Z}^\intercal S$, and $\Psi$ is a $K_2 \times K_2$ diagonal matrix with dual variables $\psi$ expanding along its diagonal. By taking the derivative with respect to $G$, we have
    \begin{equation}
      \label{eqn::dual_solution}
      G = \big( Q + \Psi \big)^{-1} W^\intercal.
    \end{equation}
    Then, by substituting Equation $\ref{eqn::dual_solution}$ into Equation \ref{eqn::Lagrangian}, we have the following dual for Equation \ref{eqn::Lagrangian}
    \begin{equation}
      \label{eqn:Lagrangian_dual}
      \mathcal{D} (\vec{\psi}) = \frac{1}{2} \tr \big(( - W \big( Q + \Psi \big)^{-1} W^\intercal - c \Psi) \big) + const.
    \end{equation}
    The gradient $\nabla$ and Hessian $H$ of the above dual with respect to $\vec{\psi}$ can be derived as follows:
    \begin{eqnarray*}
      \nabla_i = \frac{\partial \mathcal{D} (\vec{\psi})}{\partial \psi_i} &=& \frac{1}{2} \| W ( Q + \Psi )^{-1} e_i \big\|^2 - \frac{1}{2} c.\\
      H_{ij} = \frac{\partial^2 \mathcal{D} (\vec{\psi})}{\partial \psi_i \partial \psi_j} &=& - \big(( Q + \Psi )^{-1} W^\intercal W ( Q + \Psi )^{-1} \big)_{i, j} \big( ( Q + \Psi )^{-1} \big)_{i,j}.
    \end{eqnarray*}
    The Newton's method is used to optimize Equation \ref{eqn:Lagrangian_dual} with respect to $\Psi$. Thus, the update for $\Psi$ at iteration $t$ can be written as
    \begin{equation}
      \label{eqn::Newton_solution}
      \Psi^{(t)} = \Psi^{(t-1)} - (H^{(t-1)})^{-1} \nabla^{(t-1)}, 
    \end{equation}
    where $\Psi^{(t-1)}, H^{(t-1)}, \nabla^{(t-1)}$ are the diagonal matrix of $\psi$, gradient, and Hessian matrix at iteration $t-1$. In practice, we alternatively compute the updates for $G,\Psi$ with Equation \ref{eqn::dual_solution} and \ref{eqn::Newton_solution}, respectively, until the sum of squared difference in $\Psi$ between two consecutive iterations less than a predefined threshold ($10^{-4}$ is used in our studies). Further details on the derivation of Lagrange dual are provided in $A.1$ in the Appendices.

    When optimizing Equation \ref{eqn::sc_rna_obj_mat} with respect to $s_i$, the $i$-th row of $S$, our objective function with respect to $s_i$ can be simplified to 
    \begin{equation}
      \label{eqn::solve_coding}
      \mathcal{L} (s_i) = \frac{1}{2} \| \tilde{Z_i} - G^T s_i \|_2^2 + \frac{1}{2} \lambda (1 - \alpha)\|s_i\|_2^2 + \lambda \alpha |s_i|_1.
    \end{equation}
    Here $\tilde{Z} = Z - X_D D V - X_P P V$, and $\tilde{Z}_i$ is the vector of the $i$-th row of $\tilde{Z}$. Random coordinate descent (RCD) with strong rules is proposed in Algorithm 1 in Study \cite{kai2022insider} to solve the problem.
  
  \subsubsection*{Optimize with the batch strategy}
    When the scale of data is huge, optimizing SC2 with the whole data is memory demanding. To relieve this issue, we propose a batch strategy to optimize SC2. In practice, we split the whole data into several batches and optimize our object function with one batch each time to lower the memory consumption.
    
    \begin{algorithm}
      \footnotesize
      \caption{Batch SC2}\label{alg:batch_SC2}
      \KwData {$Z \in \mathbb{R}^{N \times M}$, $V_0 \in \mathbb{R}^{K_1 \times M}$, $V_0 \in \mathbb{R}^{K_1 \times M}$(random initiation), $G_0 \in \mathbb{R}^{K_2 \times M}$ (random initiation), $X_D^{N \times N_1}$, $X_P^{N \times N_2}$, T (maximum of iterations), $K$ (number of batches)}
      \KwResult{$V, G$}
      \emph{Divide $Z \in \mathbb{R}^{N \times M}$ into $K$ batches}\;
      \For{$t \leftarrow 0$ \KwTo $T - 1$}{
        \eIf{t $==$ 0}{$D_0 \leftarrow 0, P_0 \leftarrow 0, S_0 \leftarrow 0, A_0 \leftarrow 0, B_0 \leftarrow 0, E_0 \leftarrow 0, F_0 \leftarrow 0$;}{$D_0 \leftarrow D_K, P_0 \leftarrow P_K, S_0 \leftarrow S_K, A_0 \leftarrow A_K, B_0 \leftarrow B_K, E_0 \leftarrow E_K, F_0 \leftarrow F_K$;}
        \For{$k \leftarrow 1$ \KwTo $K$}{
          $\tilde{Z}_k \leftarrow Z_k - S_{k-1} G_{k-1}$\;
          Compute with a closed form similar to Equation \ref{eqn::updating_dj}
          \begin{equation*}
            D_k \triangleq \mathrm{argmin}_{D} \big \| \tilde{Z}_k - X_D^k D V_{k-1} - X_P^k P_{k-1} V_{k-1} \big \|_F^2 + \lambda_1 \|D\|_F^2.
          \end{equation*}\\
          Compute with a closed form similar to Equation \ref{eqn::updating_pt}
          \begin{equation*}
            P_k \triangleq \mathrm{argmin}_{P} \big \| \tilde{Z}_k - X_D^k D_k V_{k-1} - X_{P_k} P V_{k-1} \big \|_F^2 + \lambda_1 \|P\|_F^2.
          \end{equation*}\\
          $A_k \leftarrow A_{k-1} + (X_D^k D_k + X_P^k P_k)^\intercal (X_D^k D_k + X_P^k P_k)$, $B_k \leftarrow B_{k-1} + \tilde{Z}_k^\intercal (X_D^k D_k + X_P^k P_k)$\;
          Compute with a closed form similar to Equation \ref{eqn::updating_vm}
          \begin{equation*}
            \begin{split}
              V_k \triangleq & \mathrm{argmin}_{V} \frac{1}{k} \sum_{j=1}^k \big \| \tilde{Z}_j - (X_D^j D_j + X_P^j P_j) V \big \|_F^2 + \lambda_1 \|V\|_F^2 \\
              = & \mathrm{argmin}_{V} \frac{1}{k} \tr(V^\intercal A_k V) - \frac{2}{k} \tr(B_k V) + \lambda_1 \|V\|_F^2 .
            \end{split}
          \end{equation*}\\
          $\tilde{Z}_k \leftarrow Z_k - (X_D^k D_k + X_P^k P_k) V_k$\;
          Compute with RCD with strong rules
          \begin{equation*}
            S_k \triangleq \frac{1}{2} \mathrm{argmin}_{S} \big \| \tilde{Z}_k - S G_{k-1} \big \|_F^2 + \lambda_2 \bigg[ \frac{1}{2} (1 - \alpha) \|S_i\|_F^2 + \alpha \|S_i\|_1 \bigg].
          \end{equation*}\\
          $E_k \leftarrow E_{k-1} + S_k^\intercal S_k$, $F_k \leftarrow F_{k-1} + \tilde{Z}_k^\intercal S_k$\;
          Compute with Lagrange dual
          \begin{equation*}
            \begin{split}
              G_k \triangleq & \mathrm{argmin}_{G} \frac{1}{k} \sum_{j=1}^k \frac{1}{2}\big \| \tilde{Z}_j - S_j G \big \|_F^2 \quad \mathrm{s.t.} \, \|G_i\|_2^2 \leq c, \forall i = 1, \dots, K_2 \\
              = & \mathrm{argmin}_{G} \frac{1}{k} \sum_{j=1}^k \frac{1}{2} \tr \big(G^\intercal E_j G \big) - \tr \big( F_j G \big) \quad \mathrm{s.t.} \, \|G_i\|_2^2 \leq c, \forall i = 1, \dots, K_2.
            \end{split}
          \end{equation*}\\
        }
      }
    \end{algorithm}    

    The key to perform batch optimization of SC2 is to come up with a \textit{surrogate} that asymptotically converges to the same solution defined by Equation \ref{eqn::sc_rna_obj_mat}. As inspired by a previous study \cite{mairal2009online}, we come up with the following \textit{surrogate} for our objective
    \begin{equation}
      \label{eqn::sc_obj_surrogate}
      \begin{split}
        \ell (V, G) = & \frac{1}{k} \sum_j^k \frac{1}{2} \big \| Z_j -  (X_{D_j} D_j + X_{P_j} P_j) V - S_j G \big \|_F^2 + \\
         & \frac{1}{k} \sum_j^k \frac{1}{2} \lambda_1 (\|D_j\|_F^2 + \|P_j\|_F^2) + \frac{1}{2} \lambda_1 \|V\|_F^2 + \\
         & \frac{1}{k} \sum_j^k \lambda_2 \big[ \frac{1}{2} (1 - \alpha) \|S_j\|_F^2 + \alpha \|S_j\|_1 \big], 
      \end{split}
    \end{equation}
    Here $k$ is the number of batches, and $P_j, D_j, S_j$ are obtained with previous batches. Algorithm \ref{alg:batch_SC2} is proposed to optimize the above \textit{surrogate}. 

    In Algorithm \ref{alg:batch_SC2}, we note that $A_t, B_t, E_t, F_t$ carry all information from the past iteration. In particularly, these matrices can carry  "old" information for the same batch in different iterations. Actually, this kind of information is outdated. Mairal et al. suggested that one can accelerate convergence by removing old information for the same batch from these matrices \cite{mairal2009online}. Specifically, owning to the design of our algorithm, we use the following equations to exploit this idea
    \begin{equation}
      \label{eqn::sc_removing_old_info}
      \begin{split}
        A_k \leftarrow & A_{k-1} - (X_D^k D'_k + X_P^k P'_k)^\intercal (X_D^k D'_k + X_P^k P'_k) \\
        B_k \leftarrow & B_{k-1} - \tilde{Z'_k}^\intercal (X_D^k D'_k + X_P^k P'_k) \\
        E_k \leftarrow & E_{k-1} - {S'_k}^\intercal S'_k \\
        F_k \leftarrow & F_{k-1} - \tilde{Z'_k}^\intercal S'_k.
      \end{split}
    \end{equation}
    Here $D'_k, P'_k, S'_k$ are the corresponding matrices from the previous iteration $t-1$. With slight abuse of notation, ${\tilde{Z'}_k}$ in the second and fourth line are computed with equations in lines 9 and 14 in Algorithm \ref{alg:batch_SC2}, respectively.
    
  
  \subsection{Initialization, hyperparameter tuning, and the stopping criteria}
    In SC2, all latent variables ($D, P, V, S, G$) are initiated from normal distribution $N(0, 0.001)$. For the initial start of $s_i$ in solving subproblems defined by Equation \ref{eqn::solve_coding}, we consider the solution from ridge regression or the solution for $s_i$  from the previous iteration, depending on which one leads to a lower loss in the objective defined by Equation \ref{eqn::solve_coding}. Following the subproblem defined by Equation \ref{eqn::solve_coding}, the warm start with ridge solution is as follows
    \begin{equation*}
      s_i = [G G^T + \lambda (1 - \alpha) \mathbb{I}_{K_2} ]^{-1} G \tilde{Z}_i.
    \end{equation*}

    For model selection in SC2, grid search is utilized to select hyperparameters $\lambda_1, \lambda_2, \alpha, K_1, K_2$. In practice, when the number of observations is huge (e.g., $\geq$ 500,000), we randomly and evenly draw a small proportion (e.g., 0.1 or even less) of scRNA-Seq samples as dataset for model selection. Then, we randomly draw $10\%$ elements from the matrix of the dataset drawn for model selection as testset and select the set of hyperparameters that performs the best in terms of root-mean-square error (RMSE) on the testset. For each set of candidate hyperparameters, we run alternating BCD a number of times (e.g., 20) and choose the one with the best performance on the testset. 

    In practice, we find that SC2 always chooses $\alpha$ equal 1 in model selection, that is, SC2 favors Lasso penalty in applications. Thus, to simplify the parameter tuning for SC2, we set $\alpha$ equal 1. Meanwhile, we also consider that our two hyperparameters $K_1$ and $\lambda_1$ are kind of redundant, since one can increase $K_1$ and $\lambda_1$ simultaneously without changing the model complexity. Therefore, we consider $K_1 = K_2$ in model selection.

    The detailed procedure for model selection is as follows. First, we set $\lambda_1, \lambda_2$ to a small number (e.g., $0.1$) to avoid singularity in matrix inverse and $\alpha$ is fixed to $1$ and choose the ranks of latent representations $K_1 = K_2$ from sequences from 5 to 30 with step size 2. Then, after choosing the ranks of latent representations, we define a broad parameter grid for $\lambda_1, \lambda_2$ and perform a grid search to tuning hyperparameters. We may also refine the parameter grid based on the performance of our parameter sets on the testset. Finally, we select the parameters $\lambda_1, \lambda_2$ with the best performance on the testset and run SC2 with the selected parameters until stopping criteria meets.

    In our study, the loss in the $i$-th iteration $\ell_i (\cdot)$ is calculated every 10 iterations to reduce computational burden. Then, the stopping criteria is defined as
    \begin{equation}
      \frac{|\ell_i (\cdot) - \ell_{i-10} (\cdot)|}{\ell_{i-10} (\cdot)} < \sigma,
    \end{equation}
    where $\sigma$ is a predefined threshold and set to $10^{-8}$ in our experiment.

\newpage
\printbibliography

\newpage
\begin{appendices}
\section{Appendices}
  \subsection{Lagrange dual for learning bases}
  Here we derive the updates of bases $B$ in the problem defined by the following:
  \begin{equation*}
      \mathrm{minimize} \qquad \|Z - S G\|_F^2 + \lambda \|S\|_1 \qquad \mathrm{subject\  to}\ \|G_i\|_2^2 \leq 1, \forall i = 1, \dots, k, 
  \end{equation*} 
  where $Z$ is the matrix to be approximated, $S$ is the sparse coding, and $G$ is the matrix for the bases. In the equation, $G_i$ is the $i$-th row of $G$.

  To solve the problem, we consider the following Lagrangian:
  \begin{equation}
    \label{eqn::lagrangian}
    \begin{split}
      \mathcal{L} (G, \vec{\lambda}) & = \frac{1}{2} \tr \big( (Z - S G)^\intercal (Z - S G) \big) + \frac{1}{2} \sum_k \lambda_k  ( \sum_j G_{kj}^2 - 1)\\
      & = \frac{1}{2} \tr \big( (Z - S G)^\intercal (Z - S G) \big) + \frac{1}{2} \tr(\Lambda G G^\intercal - \Lambda), 
    \end{split}
  \end{equation}
  where each $\lambda_k \geq 0$ is a dual variable and $\Lambda = \diag(\vec{\lambda})$. By taking derivative with respect to $G$, we obtain
  \begin{equation*}
    \frac{\partial \mathcal{L} (G, \vec{\lambda})}{\partial G} = - Z^\intercal S + S^\intercal S G + \Lambda G   = 0. 
  \end{equation*}
  Here $F=Z^\intercal S$ and $E = S^\intercal S$. With these notations, we have 
  \begin{equation}
    \label{eqn::base}
    G = \big( E + \Lambda \big)^{-1} F^\intercal.
  \end{equation}
  By replacing Equation \ref{eqn::base} into the Lagrangian \ref{eqn::lagrangian}, we can further derive the Lagrange dual for our problem:
  \begin{equation*}
    \begin{split}
      \mathcal{D} (\vec{\lambda}) = & \min_G \mathcal{L} (G, \vec{\lambda}) = \frac{1}{2} \tr \big(( Z^\intercal Z - 2 F G +  G^\intercal E G + \Lambda G G^\intercal - \Lambda)\big) \\
      = & \frac{1}{2} \tr \big(( Z^\intercal Z - 2 F \big( E + \Lambda \big)^{-1} F^\intercal + F \big( E + \Lambda \big)^{-1} E \big( E + \Lambda \big)^{-1} F^\intercal + \\
      & F \big( E + \Lambda \big)^{-1} \Lambda \big( E + \Lambda \big)^{-1} F^\intercal - \Lambda) \big)\\
      = & \frac{1}{2} \tr \big(( Z^\intercal Z - 2 F \big( E + \Lambda \big)^{-1} F^\intercal + F  \big( E + \Lambda \big)^{-1} F^\intercal - \Lambda) \big)\\
      = & \frac{1}{2} \tr \big(( Z^\intercal Z -  F \big(E + \Lambda \big)^{-1} F^\intercal - \Lambda) \big),
    \end{split}
  \end{equation*}
  which is formulated as
  \begin{equation}
    \label{eqn::dual}
    \mathcal{D} (\vec{\lambda}) = \frac{1}{2} \tr \big(( Z^\intercal Z -  F \big( E + \Lambda \big)^{-1} F^\intercal - \Lambda) \big).
  \end{equation}
  The gradient and Hessian of $\mathcal{D} (\vec{\lambda})$ are computed as follows:
  \begin{eqnarray}
    \frac{\partial \mathcal{D} (\vec{\lambda})}{\partial \lambda_i} &= & \frac{1}{2} \| F ( E + \Lambda )^{-1} e_i \big\|^2 - \frac{1}{2}.\\
    \frac{\partial^2 \mathcal{D} (\vec{\lambda})}{\partial \lambda_i \partial \lambda_j} &=& - \big( ( E + \Lambda )^{-1} F^\intercal F ( E + \Lambda )^{-1} \big)_{i, j} \big( (E + \Lambda )^{-1} \big)_{i,j}.
  \end{eqnarray}
  The Newton's method is used to optimize the above problem.


\end{appendices}
\end{document}